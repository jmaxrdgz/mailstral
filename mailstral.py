# -*- coding: utf-8 -*-
"""mailstral.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NaUGRzit4ppnnpp3aOdzWV3iqJOnNpqR
"""

!pip install -qq mistral_inference torch

from huggingface_hub import snapshot_download, login
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

login()

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)

import torch
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest

!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path).half()

completion_request = ChatCompletionRequest(messages=[UserMessage(content="Be concise. What is the max length for an input prompt on Mistral7B ? How does that translate roughly to number of words ?")])
tokens = tokenizer.encode_chat_completion(completion_request).tokens
out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.3, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])
print(result)